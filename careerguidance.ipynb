{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12498260,"sourceType":"datasetVersion","datasetId":7887752}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:10:12.336217Z","iopub.execute_input":"2025-07-17T13:10:12.336897Z","iopub.status.idle":"2025-07-17T13:10:12.349572Z","shell.execute_reply.started":"2025-07-17T13:10:12.336871Z","shell.execute_reply":"2025-07-17T13:10:12.348956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# --- 1. Load all datasets ---\n# Using content fetched from previous step \ntry:\n    df_career_pred = pd.read_csv('/kaggle/input/unprocessedcareerguidancedata/career_pred.csv')\n    df_career_compass = pd.read_csv('/kaggle/input/unprocessedcareerguidancedata/career_pred.csv')\n    df_career_qa = pd.read_csv('/kaggle/input/unprocessedcareerguidancedata/Career20QA20Dataset.csv')\n    df_cs_students = pd.read_csv('/kaggle/input/unprocessedcareerguidancedata/cs_students.csv')\n    df_career_dataset_excel_sheet = pd.read_excel('/kaggle/input/unprocessedcareerguidancedata/Career Dataset.xlsx')\n    df_dataset9000 = pd.read_csv('/kaggle/input/unprocessedcareerguidancedata/dataset9000.csv')\n\n    print(\"All datasets loaded successfully.\")\n\nexcept FileNotFoundError as e:\n    print(f\"Error loading file: {e}. Please ensure all CSV files are in the correct directory.\")\n    exit() # Exit if essential files are not found\n\n# --- 2. Preprocessing and Standardization ---\n\n# --- 2.1. Standardize Skill and Role Names ---\n# This is a critical manual step. You need to inspect unique values across relevant columns\n# and define consistent mappings. This is illustrative; actual mappings would be more extensive.\n\n# Example mapping for common skill categories from dataset9000 and career_pred\n# You'd expand this greatly after full data exploration\nskill_mapping = {\n    'Database Fundamentals': 'Database_Skills',\n    'Computer Architecture': 'Computer_Architecture_Skills',\n    'Distributed Computing Systems': 'Distributed_Computing_Skills',\n    'Cyber Security': 'Cyber_Security_Skills',\n    'Networking': 'Networking_Skills',\n    'Software Development': 'Software_Development_Skills',\n    'Programming Skills': 'Programming_Skills', # General programming\n    'Project Management': 'Project_Management_Skills',\n    'Computer Forensics Fundamentals': 'Computer_Forensics_Skills',\n    'Technical Communication': 'Technical_Communication_Skills',\n    'AI ML': 'AI_ML_Skills',\n    'Software Engineering': 'Software_Engineering_Skills',\n    'Business Analysis': 'Business_Analysis_Skills',\n    'Communication skills': 'Communication_Skills',\n    'Data Science': 'Data_Science_Skills',\n    'Troubleshooting skills': 'Troubleshooting_Skills',\n    'Graphics Designing': 'Graphics_Designing_Skills',\n    # From career_pred.csv - need to handle \"percentage in\" and direct names\n    'Acedamic percentage in Operating Systems': 'Operating_Systems_Skills',\n    'percentage in Algorithms': 'Algorithms_Skills',\n    'Percentage in Programming Concepts': 'Programming_Skills', # Map to general programming\n    'Percentage in Software Engineering': 'Software_Engineering_Skills',\n    'Percentage in Computer Networks': 'Networking_Skills',\n    'Percentage in Electronics Subjects': 'Electronics_Skills',\n    'Percentage in Computer Architecture': 'Computer_Architecture_Skills',\n    'Percentage in Mathematics': 'Mathematics_Skills',\n    'Percentage in Communication skills': 'Communication_Skills',\n    'Logical quotient rating': 'Logical_Reasoning_Skills',\n    'coding skills rating': 'Coding_Skills_Rating',\n    'public speaking points': 'Public_Speaking_Skills',\n    'reading and writing skills': 'Reading_Writing_Skills',\n    'memory capability score': 'Memory_Capability_Score',\n    # Add other soft skills/traits/experience from career_pred.csv here as new features if applicable\n    # e.g., 'Hours working per day', 'hackathons', 'can work long time before system?', etc.\n}\n\n# Example mapping for common career roles\nrole_mapping = {\n    'Database Administrator': 'Database_Professional',\n    'Hardware Engineer': 'Hardware_Engineer',\n    'Application Support Engineer': 'Application_Support_Engineer',\n    'Cyber Security Specialist': 'Cyber_Security_Specialist',\n    'Networking Engineer': 'Networking_Engineer',\n    'Software Developer': 'Software_Developer',\n    'API Specialist': 'API_Specialist',\n    'Project Manager': 'Project_Manager',\n    'Information Security Specialist': 'Information_Security_Specialist',\n    'Technical Writer': 'Technical_Writer',\n    'AI ML Specialist': 'AI_ML_Specialist',\n    'Business Analyst': 'Business_Analyst',\n    'Customer Service Executive': 'Customer_Service_Executive',\n    'Data Scientist': 'Data_Scientist',\n    'Helpdesk Engineer': 'Helpdesk_Engineer',\n    'Graphics Designer': 'Graphics_Designer',\n    # From career_pred.csv\n    'Database Developer': 'Database_Professional',\n    'Portal Administrator': 'Administrator', # General admin if specific is not found\n    'Systems Security Administrator': 'Cyber_Security_Specialist',\n    'Business Systems Analyst': 'Business_Analyst',\n    'Mobile Applications Developer': 'Mobile_App_Developer',\n    'UX Designer': 'UX_UI_Designer',\n    'Quality Assurance Associate': 'QA_Engineer',\n    'Web Developer': 'Web_Developer',\n    'Information Security Analyst': 'Information_Security_Specialist',\n    'CRM Business Analyst': 'Business_Analyst',\n    'Business Intelligence Analyst': 'Business_Intelligence_Analyst',\n    'Programmer Analyst': 'Programmer_Analyst',\n    'Technical Support': 'Technical_Support_Engineer',\n    'Design & UX': 'UX_UI_Designer',\n    'Solutions Architect': 'Solutions_Architect',\n    'Database Manager': 'Database_Professional', # Map to general database if no specific 'manager' role\n    'Information Technology Manager': 'IT_Manager',\n    'Network Engineer': 'Networking_Engineer',\n    'Technical Engineer': 'Technical_Engineer',\n    'Software Quality Assurance (QA) / Testing': 'QA_Engineer',\n    'Systems Analyst': 'Systems_Analyst',\n    'CRM Technical Developer': 'Software_Developer', # Assuming this is a type of dev\n    'Technical Services/Help Desk/Tech Support': 'Technical_Support_Engineer',\n    'Data Architect': 'Data_Architect',\n    'Application Support Engineer': 'Application_Support_Engineer',\n    'Product Manager': 'Product_Manager'\n}\n\n\n# --- 2.2. Preprocess dataset9000.csv ---\n# Convert categorical interest levels to numerical\ninterest_level_map = {\n    'Not Interested': 0,\n    'Poor': 1,\n    'Beginner': 2,\n    'Average': 3,\n    'Intermediate': 4,\n    'Excellent': 5,\n    'Professional': 6\n}\ndf_dataset9000_processed = df_dataset9000.copy()\nfor col in df_dataset9000_processed.columns[:-1]: # All columns except 'Role'\n    df_dataset9000_processed[col] = df_dataset9000_processed[col].map(interest_level_map)\n\n# Rename columns based on skill_mapping\ndf_dataset9000_processed = df_dataset9000_processed.rename(columns=skill_mapping)\n# Rename 'Role' column for consistency\ndf_dataset9000_processed = df_dataset9000_processed.rename(columns={'Role': 'Target_Role'})\ndf_dataset9000_processed['Target_Role'] = df_dataset9000_processed['Target_Role'].map(role_mapping)\n\n\n# --- 2.3. Preprocess career_pred.csv ---\ndf_career_pred_processed = df_career_pred.copy()\n\n# Convert percentage columns to numerical (already are, just rename)\nrename_cols_pred = {k: v for k, v in skill_mapping.items() if k in df_career_pred_processed.columns}\ndf_career_pred_processed = df_career_pred_processed.rename(columns=rename_cols_pred)\n\n# Convert other categorical features to numerical (e.g., 'yes'/'no' to 1/0)\nbinary_map = {'yes': 1, 'no': 0}\nfor col in ['can work long time before system?', 'self-learning capability?', 'worked in teams ever?', 'Introvert']:\n    if col in df_career_pred_processed.columns:\n        df_career_pred_processed[col] = df_career_pred_processed[col].map(binary_map)\n\n# Map target role\ndf_career_pred_processed = df_career_pred_processed.rename(columns={'Suggested Job Role': 'Target_Role'})\ndf_career_pred_processed['Target_Role'] = df_career_pred_processed['Target_Role'].map(role_mapping)\n\n# Drop potentially problematic or highly unique columns for initial merge if not handled in mapping\ncols_to_drop_pred = [\n    'Student ID', 'Name', 'Interested subjects', 'interested career area ',\n    'Job/Higher Studies?', 'Type of company want to settle in?',\n    'Taken inputs from seniors or elders', 'interested in games',\n    'Interested Type of Books', 'Salary Range Expected',\n    'In a Realtionship?', 'Gentle or Tuff behaviour?', 'Management or Technical',\n    'Salary/work', 'hard/smart worker'\n]\ndf_career_pred_processed = df_career_pred_processed.drop(columns=[col for col in cols_to_drop_pred if col in df_career_pred_processed.columns])\n\n\n# --- 2.4. Preprocess cs_students.csv ---\ndf_cs_students_processed = df_cs_students.copy()\n\n# Convert skill levels (Python, SQL, Java) to numerical\nskill_strength_map = {'Strong': 3, 'Average': 2, 'Weak': 1} # Or 0-1 scale if preferred\ndf_cs_students_processed['Python_Skills'] = df_cs_students_processed['Python'].map(skill_strength_map)\ndf_cs_students_processed['SQL_Skills'] = df_cs_students_processed['SQL'].map(skill_strength_map)\ndf_cs_students_processed['Java_Skills'] = df_cs_students_processed['Java'].map(skill_strength_map)\n\n\n# Map 'Future Career' to standardized roles\ndf_cs_students_processed = df_cs_students_processed.rename(columns={'Future Career': 'Target_Role'})\ndf_cs_students_processed['Target_Role'] = df_cs_students_processed['Target_Role'].map(role_mapping)\n\n# Drop irrelevant columns for the merged analytical dataset\ncols_to_drop_students = ['Student ID', 'Name', 'Gender', 'Age', 'Major', 'Interested Domain', 'Projects', 'Python', 'SQL', 'Java']\ndf_cs_students_processed = df_cs_students_processed.drop(columns=[col for col in cols_to_drop_students if col in df_cs_students_processed.columns])\n\n# Rename GPA to a more generic academic performance score if needed\ndf_cs_students_processed = df_cs_students_processed.rename(columns={'GPA': 'Academic_Performance_Score'})\n\n\n# --- 3. Align and Concatenate the Feature-Rich Datasets ---\n\n# Identify all unique features across processed datasets that should be in the final merged df\nall_features = set()\nfor df in [df_dataset9000_processed, df_career_pred_processed, df_cs_students_processed]:\n    # Exclude 'Target_Role' and columns clearly used for mapping that are not features themselves\n    current_features = [col for col in df.columns if col != 'Target_Role' and not col.endswith('_Skills') and col not in ['GPA', 'Academic_Performance_Score']]\n    # Add mapped skill columns\n    mapped_skills = [col for col in df.columns if '_Skills' in col or col == 'Academic_Performance_Score' or col == 'Coding_Skills_Rating']\n    all_features.update(current_features + mapped_skills)\n\n# Convert to list for consistent ordering\nall_features = sorted(list(all_features))\n\n# Add 'Target_Role' at the end\nfinal_columns = all_features + ['Target_Role']\n\n# Function to reindex and fill missing columns for each DataFrame\ndef align_dataframe_columns(df, target_columns):\n    # Separate features and target for alignment\n    df_features = df.drop(columns=['Target_Role'], errors='ignore')\n    df_target = df['Target_Role'] if 'Target_Role' in df.columns else None\n\n    # Identify common and unique columns after initial processing steps\n    aligned_features = {}\n    for col in target_columns:\n        if col != 'Target_Role': # Don't try to fill target column with features\n            if col in df_features.columns:\n                aligned_features[col] = df_features[col]\n            else:\n                # Fill missing numerical features with 0, boolean with 0, and others with NaN/None\n                # This needs careful consideration based on data types and meaning\n                # For simplicity, filling numeric-like skills/ratings with 0 (Not Interested)\n                # For other types, fill with a sensible default or leave as NaN.\n                if 'Skills' in col or 'Rating' in col or 'Score' in col:\n                    aligned_features[col] = 0 # Assuming 0 for 'Not Interested' in skills\n                else:\n                    aligned_features[col] = np.nan # For other non-directly mapped numeric features\n\n    # Recreate DataFrame with aligned columns\n    aligned_df = pd.DataFrame(aligned_features, columns=[c for c in target_columns if c != 'Target_Role'])\n\n    # Add back the target column if it existed\n    if df_target is not None:\n        aligned_df['Target_Role'] = df_target\n\n    return aligned_df\n\n# Align all three processed dataframes\naligned_df_dataset9000 = align_dataframe_columns(df_dataset9000_processed, final_columns)\naligned_df_career_pred = align_dataframe_columns(df_career_pred_processed, final_columns)\naligned_df_cs_students = align_dataframe_columns(df_cs_students_processed, final_columns)\n\n# Concatenate them\nmerged_df_analytical = pd.concat([aligned_df_dataset9000, aligned_df_career_pred, aligned_df_cs_students], ignore_index=True)\n\n# Drop rows where 'Target_Role' is NaN (due to unmatched roles or initial mapping issues)\nmerged_df_analytical.dropna(subset=['Target_Role'], inplace=True)\n\nprint(f\"\\nMerged Analytical Dataset Shape: {merged_df_analytical.shape}\")\nprint(\"\\nSample of Merged Analytical Dataset:\")\nprint(merged_df_analytical.head())\n\n# --- 4. Role of Q&A and Lookup Datasets ---\nprint(\"\\n--- Understanding the role of other datasets ---\")\nprint(\"`Career Dataset.xlsx - Sheet1.csv` is a lookup table mapping careers to skills. It can be used to refine or validate the 'skill_mapping' and 'role_mapping' definitions manually, or to generate synthetic data based on career-skill relationships.\") \nprint(\"`CareerCompassDataset.csv` contains Q&A about the Career Compass platform and some role descriptions. This data is primarily textual and would be used for Natural Language Processing (NLP) tasks, like building a conversational AI, or extracting more nuanced textual features (e.g., using embeddings) for the predictive model, rather than direct tabular merging.\") \nprint(\"`Career QA Dataset.csv` also contains Q&A about various roles. Similar to `CareerCompassDataset.csv`, it's best utilized for NLP or as a knowledge base, not for direct tabular merging into the numerical prediction dataset.\") \nprint(\"\\nThese Q&A datasets provide rich contextual information that can enhance the user experience of a 'CS Compass' platform, and could serve as data for fine-tuning large language models for career guidance, but they don't directly merge into the primary numerical dataset for predictive modeling without significant feature engineering (e.g., text embeddings).\")\n\n# --- 5. Save the merged dataset to a CSV file for download ---\n# The index=False argument prevents pandas from writing the DataFrame index as a column\nmerged_df_analytical.to_csv('merged_analytical_data.csv', index=False)\n\nprint(\"\\nDataset saved successfully to 'merged_analytical_data.csv'.\")\nprint(\"You can now download it from the output directory.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:10:12.434502Z","iopub.execute_input":"2025-07-17T13:10:12.434716Z","iopub.status.idle":"2025-07-17T13:10:13.263718Z","shell.execute_reply.started":"2025-07-17T13:10:12.434700Z","shell.execute_reply":"2025-07-17T13:10:13.262832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\n# --- 1. Load the Dataset ---\ntry:\n    df = pd.read_csv('/kaggle/working/merged_analytical_data.csv')\n    print(\"Dataset loaded successfully.\")\n    print(f\"Original dataset shape: {df.shape}\")\nexcept FileNotFoundError:\n    print(\"Error: 'merged_analytical_data.csv' not found. Please ensure it's in the correct directory.\")\n    exit()\n\n# --- 2. Prepare Data and Identify Column Types ---\n\n# Separate features (X) and target variable (y)\nX = df.drop('Target_Role', axis=1)\ny = df['Target_Role']\n\n# Identify numerical and categorical columns\n# We will treat columns with dtype 'object' as categorical.\n# All other columns (int64, float64) will be treated as numerical.\nnumerical_features = X.select_dtypes(include=np.number).columns.tolist()\ncategorical_features = X.select_dtypes(include=['object']).columns.tolist()\n\nprint(f\"\\nIdentified {len(numerical_features)} numerical features.\")\nprint(f\"Identified {len(categorical_features)} categorical features.\")\n\n\n# --- 3. Define the Preprocessing Pipelines ---\n\n# Create a pipeline for numerical features\n# Step 1: Impute missing values with the median of the column. Median is robust to outliers.\n# Step 2: Scale the features to have a mean of 0 and a standard deviation of 1.\nnumerical_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# Create a pipeline for categorical features\n# Step 1: Impute missing values with the most frequent value (the mode).\n# Step 2: One-hot encode the features. 'handle_unknown='ignore'' prevents errors\n#         if a category appears in the test set but not the training set.\ncategorical_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n])\n\n# Combine the numerical and categorical pipelines into a single preprocessor\n# This object will apply the correct transformation to each column type.\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_pipeline, numerical_features),\n        ('cat', categorical_pipeline, categorical_features)\n    ],\n    remainder='passthrough' # Keep other columns if any (should be none in this case)\n)\n\nprint(\"\\nPreprocessing pipelines created successfully.\")\n\n\n# --- 4. Split Data and Apply the Full Pipeline with SMOTE ---\n\n# Split the data into training and testing sets BEFORE any preprocessing or resampling\n# 'stratify=y' ensures that the distribution of the target variable is the same\n# in both the training and testing sets, which is crucial for imbalanced datasets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(f\"\\nData split into training and testing sets.\")\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"X_test shape: {X_test.shape}\")\n\n# --- Apply the preprocessing ---\n# Fit the preprocessor on the training data and transform it\nX_train_processed = preprocessor.fit_transform(X_train)\n\n# ONLY transform the test data using the preprocessor fitted on the training data\n# This prevents data leakage from the test set.\nX_test_processed = preprocessor.transform(X_test)\n\nprint(\"\\nPreprocessing applied to training and testing sets.\")\nprint(f\"Shape of X_train after preprocessing: {X_train_processed.shape}\")\nprint(f\"Shape of X_test after preprocessing: {X_test_processed.shape}\")\n\n\n# --- 5. Address Class Imbalance using SMOTE ---\n\n# IMPORTANT: Apply SMOTE only to the training data to avoid data leakage.\n# The model should be tested on a non-oversampled, real-world-like distribution.\nsmote = SMOTE(random_state=42)\nprint(\"\\nOriginal training target distribution:\")\nprint(sorted(Counter(y_train).items()))\n\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n\nprint(\"\\nSMOTE applied to the training data.\")\nprint(\"\\nResampled training target distribution:\")\nprint(sorted(Counter(y_train_resampled).items()))\n\nprint(f\"\\nShape of X_train after SMOTE resampling: {X_train_resampled.shape}\")\n\n\n# --- Summary of Final, Model-Ready Datasets ---\nprint(\"\\n--- Pipeline Complete ---\")\nprint(\"You now have the following model-ready datasets:\")\nprint(f\"1. X_train_resampled: The preprocessed and oversampled training features. Shape: {X_train_resampled.shape}\")\nprint(f\"2. y_train_resampled: The oversampled training target. Shape: {y_train_resampled.shape}\")\nprint(f\"3. X_test_processed: The preprocessed testing features. Shape: {X_test_processed.shape}\")\nprint(f\"4. y_test: The original testing target. Shape: {y_test.shape}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:10:13.264994Z","iopub.execute_input":"2025-07-17T13:10:13.265485Z","iopub.status.idle":"2025-07-17T13:10:14.505402Z","shell.execute_reply.started":"2025-07-17T13:10:13.265464Z","shell.execute_reply":"2025-07-17T13:10:14.504720Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\n\n# --- 1. Load the Dataset ---\ntry:\n    df = pd.read_csv('merged_analytical_data.csv')\n    print(\"Dataset loaded successfully.\")\n    print(f\"Original dataset shape: {df.shape}\")\nexcept FileNotFoundError:\n    print(\"Error: 'merged_analytical_data.csv' not found. Please ensure it's in the correct directory.\")\n    exit()\n\n# --- 2. Prepare Data and Identify Column Types ---\nX = df.drop('Target_Role', axis=1)\ny = df['Target_Role']\n\nnumerical_features = X.select_dtypes(include=np.number).columns.tolist()\ncategorical_features = X.select_dtypes(include=['object']).columns.tolist()\n\nprint(f\"\\nIdentified {len(numerical_features)} numerical features.\")\nprint(f\"Identified {len(categorical_features)} categorical features.\")\n\n# --- 3. Define the Preprocessing Pipelines ---\nnumerical_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_pipeline, numerical_features),\n        ('cat', categorical_pipeline, categorical_features)\n    ],\n    remainder='passthrough'\n)\n\nprint(\"\\nPreprocessing pipelines created successfully.\")\n\n# --- 4. Split Data and Apply Preprocessing ---\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nX_train_processed = preprocessor.fit_transform(X_train)\nX_test_processed = preprocessor.transform(X_test)\n\nprint(\"\\nPreprocessing applied to training and testing sets.\")\n\n# --- 5. Address Class Imbalance using SMOTE ---\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n\nprint(\"\\nSMOTE applied to the training data.\")\nprint(f\"Shape of X_train after SMOTE: {X_train_resampled.shape}\")\n\n# --- 6. Encode Target Variable ---\n# XGBoost requires the target variable to be encoded as integers (0, 1, 2, ...).\nle = LabelEncoder()\ny_train_encoded = le.fit_transform(y_train_resampled)\ny_test_encoded = le.transform(y_test)\n\n# --- 7. Train the XGBoost Model on GPU ---\nprint(\"\\n--- Training XGBoost Model on GPU ---\")\n\n# To use GPUs, set device='cuda'. XGBoost will automatically detect and\n# utilize all available GPUs in the Kaggle environment.\n# 'objective': 'multi:softmax' is used for multi-class classification.\n# 'num_class': The number of unique job roles.\nxgb_classifier = xgb.XGBClassifier(\n    objective='multi:softmax',\n    num_class=len(le.classes_),\n    device='cuda',  # This is the key parameter for GPU training\n    eval_metric='mlogloss',\n    use_label_encoder=False,\n    random_state=42\n)\n\n# Train the model\nxgb_classifier.fit(X_train_resampled, y_train_encoded)\n\nprint(\"\\nModel training complete.\")\n\n# --- 8. Evaluate the Model ---\nprint(\"\\n--- Model Evaluation ---\")\n\n# Make predictions on the test set\ny_pred_encoded = xgb_classifier.predict(X_test_processed)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test_encoded, y_pred_encoded)\nprint(f\"Model Accuracy: {accuracy:.4f}\")\n\n# Decode the predictions and true labels to get the original role names\ny_pred_labels = le.inverse_transform(y_pred_encoded)\ny_test_labels = le.inverse_transform(y_test_encoded)\n\n# Display the classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_labels, y_pred_labels))\n\n# Display the confusion matrix\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(y_test_labels, y_pred_labels, labels=le.classes_)\nplt.figure(figsize=(16, 12))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\nplt.title('Confusion Matrix')\nplt.ylabel('Actual Role')\nplt.xlabel('Predicted Role')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:10:14.506220Z","iopub.execute_input":"2025-07-17T13:10:14.506507Z","iopub.status.idle":"2025-07-17T13:10:26.753217Z","shell.execute_reply.started":"2025-07-17T13:10:14.506480Z","shell.execute_reply":"2025-07-17T13:10:26.752419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport joblib # Import joblib for saving the model\n\n# --- 1. Load the Dataset ---\ntry:\n    df = pd.read_csv('merged_analytical_data.csv')\n    print(\"Dataset loaded successfully.\")\n    print(f\"Original dataset shape: {df.shape}\")\nexcept FileNotFoundError:\n    print(\"Error: 'merged_analytical_data.csv' not found. Please ensure it's in the correct directory.\")\n    exit()\n\n# --- 2. Prepare Data and Identify Column Types ---\nX = df.drop('Target_Role', axis=1)\ny = df['Target_Role']\n\nnumerical_features = X.select_dtypes(include=np.number).columns.tolist()\ncategorical_features = X.select_dtypes(include=['object']).columns.tolist()\n\nprint(f\"\\nIdentified {len(numerical_features)} numerical features.\")\nprint(f\"Identified {len(categorical_features)} categorical features.\")\n\n# --- 3. Define the Preprocessing Pipelines ---\nnumerical_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_pipeline, numerical_features),\n        ('cat', categorical_pipeline, categorical_features)\n    ],\n    remainder='passthrough'\n)\n\nprint(\"\\nPreprocessing pipelines created successfully.\")\n\n# --- 4. Split Data and Apply Preprocessing ---\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nX_train_processed = preprocessor.fit_transform(X_train)\nX_test_processed = preprocessor.transform(X_test)\n\nprint(\"\\nPreprocessing applied to training and testing sets.\")\n\n# --- 5. Address Class Imbalance using SMOTE ---\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n\nprint(\"\\nSMOTE applied to the training data.\")\nprint(f\"Shape of X_train after SMOTE: {X_train_resampled.shape}\")\n\n# --- 6. Encode Target Variable ---\nle = LabelEncoder()\ny_train_encoded = le.fit_transform(y_train_resampled)\ny_test_encoded = le.transform(y_test)\n\n# --- 7. Hyperparameter Tuning with RandomizedSearchCV ---\nprint(\"\\n--- Performing Hyperparameter Tuning ---\")\n\n# Define the parameter grid to search\n# These are some of the most important hyperparameters for XGBoost\nparam_grid = {\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'n_estimators': [100, 200, 300, 500],\n    'max_depth': [3, 5, 7, 10],\n    'subsample': [0.7, 0.8, 0.9, 1.0],\n    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n    'gamma': [0, 0.1, 0.2]\n}\n\n# Initialize the XGBoost classifier for GPU\nxgb_classifier = xgb.XGBClassifier(\n    objective='multi:softmax',\n    num_class=len(le.classes_),\n    device='cuda',\n    eval_metric='mlogloss',\n    use_label_encoder=False,\n    random_state=42\n)\n\n# Set up RandomizedSearchCV\n# n_iter: Number of parameter settings that are sampled.\n# cv: Number of folds for cross-validation.\n# verbose=2: Prints updates as it runs.\n# n_jobs=-1: Uses all available CPU cores for the search.\nrandom_search = RandomizedSearchCV(\n    estimator=xgb_classifier,\n    param_distributions=param_grid,\n    n_iter=25,  # You can increase this for a more thorough search\n    cv=3,\n    verbose=2,\n    random_state=42,\n    n_jobs=-1\n)\n\n# Fit the random search to the data\nrandom_search.fit(X_train_resampled, y_train_encoded)\n\nprint(\"\\nHyperparameter tuning complete.\")\nprint(f\"Best Parameters Found: {random_search.best_params_}\")\n\n# Get the best model found by the search\nbest_model = random_search.best_estimator_\n\n# --- 8. Evaluate the Best Model ---\nprint(\"\\n--- Evaluating Best Model ---\")\n\n# Make predictions on the test set using the best model\ny_pred_encoded = best_model.predict(X_test_processed)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test_encoded, y_pred_encoded)\nprint(f\"Tuned Model Accuracy: {accuracy:.4f}\")\n\n# Decode the predictions and true labels to get the original role names\ny_pred_labels = le.inverse_transform(y_pred_encoded)\ny_test_labels = le.inverse_transform(y_test_encoded)\n\n# Display the classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_labels, y_pred_labels))\n\n# Display the confusion matrix\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(y_test_labels, y_pred_labels, labels=le.classes_)\nplt.figure(figsize=(16, 12))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\nplt.title('Confusion Matrix for Tuned Model')\nplt.ylabel('Actual Role')\nplt.xlabel('Predicted Role')\nplt.show()\n\n# --- 9. Save the Model and Components for Future Use ---\nprint(\"\\n--- Saving Model and Components ---\")\n\n# Save the trained model\njoblib.dump(best_model, 'xgboost_career_model.pkl')\nprint(\"Trained model saved to 'xgboost_career_model.pkl'\")\n\n# Save the preprocessor\njoblib.dump(preprocessor, 'preprocessor.pkl')\nprint(\"Preprocessor saved to 'preprocessor.pkl'\")\n\n# Save the label encoder\njoblib.dump(le, 'label_encoder.pkl')\nprint(\"Label encoder saved to 'label_encoder.pkl'\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T13:21:24.708619Z","iopub.execute_input":"2025-07-17T13:21:24.709301Z","iopub.status.idle":"2025-07-17T14:04:08.076342Z","shell.execute_reply.started":"2025-07-17T13:21:24.709274Z","shell.execute_reply":"2025-07-17T14:04:08.075636Z"}},"outputs":[],"execution_count":null}]}